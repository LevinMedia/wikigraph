# Architecture & Data Flow

## Overview

This project uses a **direct database access pattern** where the frontend reads directly from Supabase, while the backend handles all writes and crawling operations.

## Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚
â”‚  (Next.js)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                 â”‚
       â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supabase   â”‚                  â”‚   Backend   â”‚
â”‚  PostgreSQL  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  (FastAPI) â”‚
â”‚              â”‚                   â”‚             â”‚
â”‚  - pages     â”‚                   â”‚  - Crawler â”‚
â”‚  - links     â”‚                   â”‚  - API     â”‚
â”‚  - page_fetchâ”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
       â–²                                  â”‚
       â”‚                                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              (Real-time subscriptions)
```

## Detailed Flow

### 1. Reading Data (Frontend â†’ Supabase)

**Graph Data:**
```
Frontend â†’ Supabase Client â†’ pages table
                          â†’ links table
```

**Jobs:**
```
Frontend â†’ Supabase Client â†’ page_fetch table
```

**Real-time Updates:**
```
Backend writes to Supabase â†’ Supabase triggers change
                          â†’ Frontend subscription receives update
                          â†’ UI updates automatically
```

### 2. Writing Data (Frontend â†’ Backend â†’ Supabase)

**Enqueueing a Page:**
```
1. Frontend â†’ POST /api/admin/enqueue
2. Backend â†’ Resolves Wikipedia title to page_id
3. Backend â†’ Inserts into pages table
4. Backend â†’ Inserts into page_fetch table (status: 'queued')
5. Backend â†’ Returns page info to frontend
6. Supabase â†’ Notifies frontend via real-time subscription
7. Frontend â†’ UI updates with new job
```

**Crawling Process:**
```
1. Backend crawler â†’ Claims job from page_fetch (status: 'running')
2. Backend crawler â†’ Fetches ALL outbound links from Wikipedia API
3. Backend crawler â†’ Batch resolves link titles to page_ids
4. Backend crawler â†’ Upserts pages into pages table
5. Backend crawler â†’ Inserts links into links table
6. Backend crawler â†’ Updates page_fetch (status: 'done')
7. Supabase â†’ Notifies frontend via real-time subscription
8. Frontend â†’ UI updates with completed job
```

## Why This Architecture?

### âœ… Benefits

1. **Performance**: Frontend reads directly from database (no API hop)
2. **Real-time**: Supabase subscriptions provide instant updates
3. **Scalability**: Backend focuses on crawling, frontend handles reads
4. **Simplicity**: No need to proxy all reads through backend API

### ðŸ”’ Security

- **Backend**: Uses direct PostgreSQL connection (service role equivalent)
- **Frontend**: Uses Supabase anon key (Row Level Security can be added)
- **API**: Only used for enqueueing (can add auth if needed)

## Environment Variables

### Backend
- `SUPABASE_DB_URL` - Direct PostgreSQL connection (writes)

### Frontend
- `NEXT_PUBLIC_SUPABASE_URL` - Supabase project URL (reads)
- `NEXT_PUBLIC_SUPABASE_ANON_KEY` - Supabase anon key (reads)
- `NEXT_PUBLIC_API_URL` - Backend API URL (enqueueing only)

## Database Tables

### `pages`
- Stores all Wikipedia pages we've seen
- Fields: `page_id`, `title`, `namespace`, `out_degree`, `in_degree`

### `links`
- Stores directed links between pages
- Fields: `from_page_id`, `to_page_id`

### `page_fetch`
- Tracks crawl jobs
- Fields: `page_id`, `status`, `priority`, `last_error`

## API Endpoints

### Backend API (FastAPI)

**POST `/api/admin/enqueue`**
- Enqueues a page for crawling
- Called by frontend
- Returns page info

**GET `/api/admin/jobs`** (legacy - not used by frontend)
- Returns job list
- Frontend now reads directly from Supabase

**GET `/api/graph/ego`** (legacy - not used by frontend)
- Returns ego graph
- Frontend now queries Supabase directly

### Frontend Functions

**`fetchEgoGraph(pageId, limitNeighbors)`**
- Queries Supabase: `pages` + `links` tables
- Builds graph structure client-side

**`fetchJobs()`**
- Queries Supabase: `page_fetch` + `pages` tables
- Returns job list with page details

**`enqueuePage(title, priority)`**
- Calls backend API: `POST /api/admin/enqueue`
- Triggers crawler process

## Real-time Subscriptions

The frontend subscribes to changes on the `page_fetch` table:

```typescript
supabase
  .channel('page_fetch_changes')
  .on('postgres_changes', {
    event: '*',
    schema: 'public',
    table: 'page_fetch',
  }, () => {
    refreshJobs() // Update UI
  })
  .subscribe()
```

This means:
- When backend updates a job status â†’ Frontend updates immediately
- No polling needed
- Better user experience


